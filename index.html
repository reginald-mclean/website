<!DOCTYPE html>
<html lang="en">
<head>
 <title>Reginald McLean</title>
 <meta charset="utf-8">
 <meta name="viewport" content="width=device-width, initial-scale=1">
 <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
 <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
 <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
 <link href='https://fonts.googleapis.com/css?family=Oswald:700' rel='stylesheet' type='text/css'>
 <style>
   .container { padding: 0 15px; }
   
   .row { margin: 0; }
   
   .col-md-8, .col-md-4 {
       padding: 0 10px;
   }

   h2 { margin: 10px 0 5px 0; }
   
   p { margin: 0 0 10px 0; }
   
   .col-md-4 img {
       max-width: 85%;
       margin-bottom: 10px;
   }

   .sidebar-name {
       font-family: 'Oswald', sans-serif;
       font-size: 32px;
       margin: 5px 0;
   }

   .social-links {
       margin-top: 10px;
   }

   .social-links dd {
       margin: 5px 0;
   }

   .publication-link {
       text-decoration: none;
       color: inherit;
       display: block;
   }

   .publication-link:hover {
       text-decoration: none;
       color: inherit;
   }

   .publication-tile {
       padding: 15px;
       border: 1px solid #e0e0e0;
       border-radius: 8px;
       margin-bottom: 15px;
       background: white;
       transition: all 0.3s ease;
       box-shadow: 0 2px 4px rgba(0,0,0,0.1);
   }

   .publication-tile:hover {
       transform: scale(1.02);
       box-shadow: 0 4px 8px rgba(0,0,0,0.2);
   }

   .publication-image {
       width: 100%;
       max-height: 200px;
       object-fit: contain;
       margin-bottom: 10px;
   }

   .publication-title {
       font-size: 1.2em;
       margin: 5px 0;
   }

   .publication-authors {
       color: #666;
       margin: 5px 0;
   }

   .publication-venue {
       font-style: italic;
       color: #444;
       margin: 5px 0;
   }

   .publication-abstract {
       margin: 5px 0;
       line-height: 1.4;
   }

   .navbar { margin-bottom: 10px; }
 </style>
</head>
<body>
   <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
     <div class="container">
       <div class="navbar-header">
         <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
           <span class="sr-only">Toggle navigation</span>
           <span class="icon-bar"></span>
           <span class="icon-bar"></span>
           <span class="icon-bar"></span>
         </button>
       </div>
       <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
         <ul class="nav navbar-nav">
           <li><a href="index.html">Home</a></li>
           <li><a href="#publications">Publications</a></li> 
           <li><a href="https://drive.google.com/drive/folders/1KM2fjUNGxntDzwmawULm3IYUg-Rk7tv2?usp=drive_link">CV</a></li> 
         </ul>
       </div>
     </div>
   </nav>
   
   <div class="container">
       <div class="row">
           <div class="col-md-8">
               <h2>About Me</h2>               
               <p>I am currently a 5th year PhD Candidate at Toronto Metropolitan University, under the supervision of Nariman Farsad and Isaac Woungang. Previously, I completed my MSc in Computer Science at Brock University under the supervision of Beatrice Ombuki-Berman, and I received my BSc. in Computer Science from Trent University.</p>
               
               <h2>Research</h2>
               <p>My research aims to enable autonomous agents to acquire the ability to accomplish multiple tasks using a single policy. I am interested in deep reinforcement learning, multi-task reinforcement learning, continual/lifelong reinforcement learning, inverse reinforcement learning, and intrinsic motivation for reinforcement learning. In addition to reinforcement learning, I am interested in generative modelling, self-supervised learning, and vision-language models.</p>
               
               <h2>Past Experience</h2>
               <p>I am currently a part-time lecturer in the Computer Science Department at Brock University. Previously, I was an intern at Royal Bank of Canada working on supporting their technical infrastructure using AIOps methods. Upon the completion of my MSc, I was the Lead Machine Learning Developer at Castle Ridge Asset Management.</p>
               
               <h2 id="publications">Publications</h2>
               <a href="https://arxiv.org/abs/2503.05126" class="publication-link">
               <div class="publication-tile">
                   <img class="publication-image" src="../data/scaling.png" alt="Publication diagram">
                   <div class="publication-title"><b>Multi-Task Reinforcement Learning Enables Parameter Scaling</b></div>
                   <div class="publication-authors"><b>Reginald McLean</b>, Evangelos Chatzaroulas, Jordan Terry, Issac Woungang, Nariman Farsad, Pablo Samuel Castro</div>
                   <div class="publication-venue">Preprint.</div>
                   <div class="publication-abstract">
                       Multi-task reinforcement learning (MTRL) aims to endow a single agent with the ability to perform well on multiple tasks. Recent works have focused on developing novel sophisticated architectures to improve performance, often resulting in larger models; it is unclear, however, whether the performance gains are a consequence of the architecture design itself or the extra parameters. We argue that gains are mostly due to scale by demonstrating that naively scaling up a simple MTRL baseline to match parameter counts outperforms the more sophisticated architectures, and these gains benefit most from scaling the critic over the actor. Additionally, we explore the training stability advantages that come with task diversity, demonstrating that increasing the number of tasks can help mitigate plasticity loss. Our findings suggest that MTRL's simultaneous training across multiple tasks provides a natural framework for beneficial parameter scaling in reinforcement learning, challenging the need for complex architectural innovations. 
                   </div>
               </div>
               </a>

               <a href="https://openreview.net/forum?id=T7bA2zjobB" class="publication-link">
               <div class="publication-tile">
                   <img class="publication-image" src="../data/sal.png" alt="Publication diagram">
                   <div class="publication-title"><b>Overcoming State and Action Space Disparities in Multi-Domain, Multi-Task Reinforcement Learning</b></div>
                   <div class="publication-authors"><b>Reginald McLean</b>, Kai Yuan, Issac Woungang, Nariman Farsad, Pablo Samuel Castro</div>
                   <div class="publication-venue">Accepted at Morphology-Aware Policy and Design Learning Workshop @ CoRL 2024</div>
                   <div class="publication-abstract">
                       Current multi-task reinforcement learning (MTRL) methods have the ability to perform a large number of tasks with a single policy. However when attempting to interact with a new domain, the MTRL agent would need to be re-trained due to differences in domain dynamics and structure. Because of these limitations, we are forced to train multiple policies even though tasks may have shared dynamics, leading to needing more samples and is thus sample inefficient. In this work, we explore the ability of MTRL agents to learn in various domains with various dynamics by simultaneously learning in multiple domains, without the need to fine-tune extra policies. In doing so we find that a MTRL agent trained in multiple domains induces an increase in sample efficiency of up to 70% while maintaining the overall success rate of the MTRL agent.
                   </div>
               </div>
               </a>
               
               <a href="https://arxiv.org/abs/2405.19988" class="publication-link">
                   <div class="publication-tile">
                       <img class="publication-image" src="../data/vlc.png" alt="Publication diagram">
                       <div class="publication-title"><b>Video Language Critic: Transferable Reward Functions for Language-Conditioned Robotics</b></div>
                       <div class="publication-authors">Minttu Alakuijala, <b>Reginald McLean</b>, Isaac Woungang, Nariman Farsad, Samuel Kaski, Pekka Marttinen, Kai Yuan</div>
                       <div class="publication-venue">Accepted at Workshop on Language and Robot Learning: Language as an Interface @ CoRL 2024</div>
                       <div class="publication-abstract">
                           Natural language is often the easiest and most convenient modality for humans to specify tasks for robots. However, learning to ground language to behavior typically requires impractical amounts of diverse, language-annotated demonstrations collected on each target robot. In this work, we aim to separate the problem of what to accomplish from how to accomplish it, as the former can benefit from substantial amounts of external observation-only data, and only the latter depends on a specific robot embodiment. To this end, we propose Video-Language Critic, a reward model that can be trained on readily available cross-embodiment data using contrastive learning and a temporal ranking objective, and use it to score behavior traces from a separate actor. When trained on Open X-Embodiment data, our reward model enables 2x more sample-efficient policy training on Meta-World tasks than a sparse reward only, despite a significant domain gap. Using in-domain data but in a challenging task generalization setting on Meta-World, we further demonstrate more sample-efficient training than is possible with prior language-conditioned reward models that are either trained with binary classification, use static images, or do not leverage the temporal information present in video data.
                       </div>
                   </div>
               </a>
               <div class="publication-tile">
                   <img class="publication-image" src="../data/ecnn.jpg" alt="Publication diagram">
                   <div class="publication-title"><b>Swarm Based Algorithms for Neural Network Training</b></div>
                   <div class="publication-authors"><b>Reginald McLean</b>, Beatrice Ombuki-Berman, Andries P. Engelbrecht</div>
                   <div class="publication-venue">Accepted at 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</div>
                   <div class="publication-abstract">
			The purpose of this paper is to compare the abilities and deficiencies of various swarm based algorithms for training artificial neural networks. This paper uses seven algorithms, seven regression problems, sixteen classification problems, and four bounded activation functions to compare algorithms in regards to loss, accuracy, hidden unit saturation, and overfitting. It was found that particle swarm optimization is the top algorithm for regression problems based on loss, firefly algorithm was the top algorithm for classification problems when examining accuracy and loss. The ant colony optimization and artificial bee colony algorithms caused the least amount of hidden unit saturation, with the bacterial foraging optimization algorithm producing the least amount of overfitting.
                   </div>
               </div>

           </div>

           <div class="col-md-4">
               <img class="img-responsive" src="../data/ReggieMcLean.jpg" alt="">
               <div class="sidebar-name"><b>Reginald McLean</b></div>
               <p><b>reginald k mclean at gmail dot com</b></p>
               <p>Department of Computer Science<br>
               Toronto Metropolitan University<br>
               Toronto, Ontario<br>
               Canada</p>
               
               <div class="social-links">
                   <dd><a href="https://scholar.google.com/citations?user=gBBveasAAAAJ&hl=en&authuser=1">Google Scholar</a></dd>
                   <dd><a href="https://x.com/reggiemclean95">Twitter</a></dd>
                   <dd><a href="https://www.linkedin.com/in/reginaldmclean/">LinkedIn</a></dd>
               </div>
           </div>
       </div>
   </div>
</body>
</html>
